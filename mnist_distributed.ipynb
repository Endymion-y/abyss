{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to train a model in a distributed manner, using the well-known MNIST problem as the classification task. In particular, the between-graph replication mode is adopted. To this end, one needs to declare the computation in a loop, and in each iteration declare the same computation for different workers using function abyss_replica_device_setter(). Besides, one needs to hold the duplicate handles of the inputs and outputs (i.e. feeds and fetches) of the computation. When session.run() is called, each share of inputs should be fed, and all the outputs are used as the fetches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abyss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import mnist, input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist_data = input_data.read_data_sets('/tmp/tensorflow/mnist/input_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ps = 2\n",
    "num_worker = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = AbyssDistributedSession(['ps', 'worker'], [num_ps, num_worker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PIXELS = 28\n",
    "hidden_units = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate handles of inputs and outputs\n",
    "global_x = []\n",
    "global_y_ = []\n",
    "global_train_step = []\n",
    "\n",
    "for worker_index in range(num_worker):\n",
    "    worker_device = '/job:worker/task:%d' % (worker_index)\n",
    "    with tf.device(abyss_replica_device_setter(ps_tasks=num_ps, worker_device=worker_device, ps_device='/job:ps/cpu:0')):\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        # Variables of the hidden layer\n",
    "        with tf.name_scope('hidden'):\n",
    "            hid_w = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "            [IMAGE_PIXELS * IMAGE_PIXELS, hidden_units], stddev=1.0/IMAGE_PIXELS), name='hid_w')\n",
    "            hid_b = tf.Variable(tf.zeros([hidden_units]), name='hid_b')\n",
    "            \n",
    "        # Variables of the softmax layer\n",
    "        sm_w = tf.Variable(\n",
    "        tf.truncated_normal([hidden_units, 10], stddev=1.0/math.sqrt(hidden_units)), name='sm_w')\n",
    "        sm_b = tf.Variable(tf.zeros([10]), name='sm_b')\n",
    "        \n",
    "        # Input\n",
    "        x = tf.placeholder(tf.float32, [None, IMAGE_PIXELS * IMAGE_PIXELS])\n",
    "        y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "        global_x.append(x)\n",
    "        global_y_.append(y_)\n",
    "        \n",
    "        hid_lin = tf.nn.xw_plus_b(x, hid_w, hid_b)\n",
    "        hid = tf.nn.relu(hid_lin)\n",
    "        \n",
    "        y = tf.nn.softmax(tf.nn.xw_plus_b(hid, sm_w, sm_b))\n",
    "        cross_entropy = -tf.reduce_sum(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n",
    "        \n",
    "        opt = tf.train.AdamOptimizer(0.01)\n",
    "        train_step = opt.minimize(cross_entropy, global_step=global_step)\n",
    "        global_train_step.append(train_step)\n",
    "        \n",
    "        init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    train_feed = {}\n",
    "    for i in range(num_worker):\n",
    "        train_feed[global_x[i]], train_feed[global_y_[i]] = mnist_data.train.next_batch(100)\n",
    "    _, step = sess.run([global_train_step, global_step], feed_dict=train_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9334\n"
     ]
    }
   ],
   "source": [
    "# Test trained model\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist_data.test.images, y_: mnist_data.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
